{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trusted -> Refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by day by sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporcion para cada sentimiento del total de tweets por dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get change percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mvv = company_dataframe.select('ticker_symbol').rdd.flatMap(lambda x: x).collect()\n",
    "mvv = [row['ticker_symbol'] for row in company_dataframe.collect()]\n",
    "print('mvv:')\n",
    "print(mvv)\n",
    "print('Downloading yfinance data:')\n",
    "prices = yf.download(mvv, start=\"2014-12-31\", end=\"2020-12-31\")['Adj Close']\n",
    "price=pd.DataFrame(index=pd.date_range(start=\"2014-12-31\",end=\"2020-12-31\"))\n",
    "price.index.name='Date'\n",
    "price = pd.concat([price,prices],axis=1)\n",
    "price=price.fillna(method='ffill')\n",
    "# print(price.isnull().sum().sum()==0)\n",
    "# print(all(pd.date_range(start=\"2015-01-01\",end=\"2020-12-31\").isin(price.index)))\n",
    "change_price=price.pct_change()\n",
    "\n",
    "print('change_price.index[:10]')\n",
    "print(change_price.index[:10])\n",
    "\n",
    "print('change_price.loc[change_price.index==2014-12-31].index:')\n",
    "print(change_price.loc[change_price.index=='2014-12-31'].index)\n",
    "\n",
    "## Eliminar periodos fuera del rango de analisis:\n",
    "change_price.drop(change_price.loc[change_price.index=='2014-12-31'].index, axis=0, inplace=True)\n",
    "\n",
    "#tweet_dataframe = tweet_dataframe.join(change_price, tweet_dataframe['post_date'] == change_price.index, 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/q/48234474\n",
    "\n",
    "\n",
    "Data Preparation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import NGram, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "indexer = StringIndexer(inputCol='category', outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"sentence_tokens\")\n",
    "remove_stop_words = StopWordsRemover(inputCol=\"sentence_tokens\", outputCol=\"filtered\")\n",
    "unigrammer = NGram(n=1, inputCol=\"filtered\", outputCol=\"tokens\") \n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"hashed_tokens\")\n",
    "idf = IDF(inputCol=\"hashed_tokens\", outputCol=\"tf_idf_tokens\")\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf_tokens'], outputCol='features')\n",
    "\n",
    "data_prep_pipe = Pipeline(\n",
    "    stages=[indexer, tokenizer, remove_stop_words, unigrammer, hashingTF, idf, clean_up]\n",
    ")\n",
    "transformed = data_prep_pipe.fit(spark_df).transform(spark_df)\n",
    "clean_data = transformed.select(['label','features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()\n",
    "(training,testing) = clean_data.randomSplit([0.7,0.3], seed=12345)\n",
    "model = nb.fit(training)\n",
    "test_results = model.transform(testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting label was: {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
