{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Integrador 1\n",
    "---\n",
    "Presentado por:\n",
    "* Karla Orozco\n",
    "* Jonathan zapata\n",
    "* Juan Fernando Gallego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trusted -> Refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries:\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specific libraries:\n",
    "# import pandas as pd\n",
    "# import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @params: [JOB_NAME]\n",
    "# args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)\n",
    "# job.init(args['JOB_NAME'], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_loc = ''\n",
    "\n",
    "inputDyf = glueContext.create_dynamic_frame_from_options(\\\n",
    "    connection_type = \"s3\", \\\n",
    "    connection_options = { \n",
    "        \"paths\": [input_loc]}, \\\n",
    "    format = \"csv\",\n",
    "    format_options={\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by day by sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proporcion para cada sentimiento del total de tweets por dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get change percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mvv = company_dataframe.select('ticker_symbol').rdd.flatMap(lambda x: x).collect()\n",
    "mvv = [row['ticker_symbol'] for row in company_dataframe.collect()]\n",
    "print('mvv:')\n",
    "print(mvv)\n",
    "print('Downloading yfinance data:')\n",
    "prices = yf.download(mvv, start=\"2014-12-31\", end=\"2020-12-31\")['Adj Close']\n",
    "price=pd.DataFrame(index=pd.date_range(start=\"2014-12-31\",end=\"2020-12-31\"))\n",
    "price.index.name='Date'\n",
    "price = pd.concat([price,prices],axis=1)\n",
    "price=price.fillna(method='ffill')\n",
    "# print(price.isnull().sum().sum()==0)\n",
    "# print(all(pd.date_range(start=\"2015-01-01\",end=\"2020-12-31\").isin(price.index)))\n",
    "change_price=price.pct_change()\n",
    "\n",
    "print('change_price.index[:10]')\n",
    "print(change_price.index[:10])\n",
    "\n",
    "print('change_price.loc[change_price.index==2014-12-31].index:')\n",
    "print(change_price.loc[change_price.index=='2014-12-31'].index)\n",
    "\n",
    "## Eliminar periodos fuera del rango de analisis:\n",
    "change_price.drop(change_price.loc[change_price.index=='2014-12-31'].index, axis=0, inplace=True)\n",
    "\n",
    "#tweet_dataframe = tweet_dataframe.join(change_price, tweet_dataframe['post_date'] == change_price.index, 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Preparation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "from pyspark.ml.feature import NGram, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "indexer = StringIndexer(inputCol='category', outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"sentence_tokens\")\n",
    "remove_stop_words = StopWordsRemover(inputCol=\"sentence_tokens\", outputCol=\"filtered\")\n",
    "unigrammer = NGram(n=1, inputCol=\"filtered\", outputCol=\"tokens\") \n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"hashed_tokens\")\n",
    "idf = IDF(inputCol=\"hashed_tokens\", outputCol=\"tf_idf_tokens\")\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf_tokens'], outputCol='features')\n",
    "\n",
    "data_prep_pipe = Pipeline(\n",
    "    stages=[indexer, tokenizer, remove_stop_words, unigrammer, hashingTF, idf, clean_up]\n",
    ")\n",
    "transformed = data_prep_pipe.fit(spark_df).transform(spark_df)\n",
    "clean_data = transformed.select(['label','features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()\n",
    "(training,testing) = clean_data.randomSplit([0.7,0.3], seed=12345)\n",
    "model = nb.fit(training)\n",
    "test_results = model.transform(testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting label was: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import common libraries:\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "# Import specific libraries:\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "## @type: DataSource\n",
    "## @args: [database = \"pi1-kjj-trusted\", table_name = \"company1_csv\", transformation_ctx = \"DataSource0\"]\n",
    "## @return: company_dynamicframe\n",
    "## @inputs: []\n",
    "company_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database = \"pi1-kjj-trusted\",\n",
    "       table_name = \"company_csv\",\n",
    "       transformation_ctx = \"company_dynamicframe\")\n",
    "# company_dynamicframe.printSchema()\n",
    "\n",
    "company_tweet_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database = \"pi1-kjj-trusted\",\n",
    "       table_name = \"company_tweet_csv\")\n",
    "# company_tweet_dynamicframe.printSchema()\n",
    "\n",
    "tweet_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database = \"pi1-kjj-trusted\",\n",
    "       table_name = \"tweet_csv\")\n",
    "# tweet_dynamicframe.printSchema()\n",
    "\n",
    "company_dataframe = company_dynamicframe.toDF()\n",
    "tweet_dataframe = tweet_dynamicframe.toDF()\n",
    "company_tweet_dataframe = company_tweet_dynamicframe.toDF()\n",
    "\n",
    "# Summarize total engagement:\n",
    "# tweet_dataframe['total_engagement'] = tweet_dataframe['comment_num'] + tweet_dataframe['retweet_num'] + tweet_dataframe['like_num']\n",
    "tweet_dataframe = tweet_dataframe.withColumn('total_engagement', sum(tweet_dataframe[col] for col in ['comment_num', 'retweet_num', 'like_num']))\n",
    "\n",
    "# Get iso date (Not epoch)\n",
    "tweet_dataframe = tweet_dataframe.withColumnRenamed('post_date', 'post_datetime_epoch')\n",
    "# tweet_dataframe['post_datetime'] = pd.to_datetime(tweet_dataframe['post_datetime_epoch'],unit='s')\n",
    "# tweet_dataframe.head(2)\n",
    "from pyspark.sql import functions as f\n",
    "# https://stackoverflow.com/a/54340652\n",
    "tweet_dataframe = tweet_dataframe.withColumn('post_datetime', f.to_timestamp(tweet_dataframe.post_datetime_epoch))\n",
    "# tweet_dataframe['post_date'] = tweet_dataframe['post_datetime'].dt.date\n",
    "tweet_dataframe = tweet_dataframe.withColumn('post_date', f.to_date(tweet_dataframe.post_datetime))\n",
    "#tweet_dataframe = tweet_dataframe.withColumn('post_date', f.to_date(f.col('post_datetime')))\n",
    "\n",
    "# Join company tweets with tweets dataframe:\n",
    "# ticker_symbol_group = company_tweet_dataframe.groupBy('tweet_id')['ticker_symbol'].distinct()\n",
    "# ticker_symbol_group.rename(\"ticker_symbol_group\", inplace=True)\n",
    "ticker_symbol_group = company_tweet_dataframe.groupBy('tweet_id').agg(\n",
    "    f.collect_set(f.col('ticker_symbol')).alias('ticker_symbol_group')\n",
    ")\n",
    "print(ticker_symbol_group.show(n=5))\n",
    "\n",
    "tweet_dataframe = tweet_dataframe.join(ticker_symbol_group, tweet_dataframe['tweet_id'] == ticker_symbol_group['tweet_id'], 'left')\n",
    "print(tweet_dataframe.show(truncate=False))\n",
    "\n",
    "\n",
    "# Tokenize:\n",
    "\n",
    "#tweet_dataframe = tweet_dataframe.withColumn('body_tokenized', nltk.word_tokenize(str(col)) for col in ['body'])\n",
    "tweet_dataframe = tweet_dataframe.withColumn('body_tokenized', tweet_dataframe.select(\"body\").rdd.map(lambda x: \n",
    "    nltk.word_tokenize(str(x))).collect()\n",
    "print(tweet_dataframe.show(n=5))\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Get change percentage:\n",
    "# mvv = company_dataframe.select('ticker_symbol').rdd.flatMap(lambda x: x).collect()\n",
    "mvv = [row['ticker_symbol'] for row in company_dataframe.collect()]\n",
    "print('mvv:')\n",
    "print(mvv)\n",
    "print('Downloading yfinance data:')\n",
    "prices = yf.download(mvv, start=\"2014-12-31\", end=\"2020-12-31\")['Adj Close']\n",
    "price=pd.DataFrame(index=pd.date_range(start=\"2014-12-31\",end=\"2020-12-31\"))\n",
    "price.index.name='Date'\n",
    "price = pd.concat([price,prices],axis=1)\n",
    "price=price.fillna(method='ffill')\n",
    "# print(price.isnull().sum().sum()==0)\n",
    "# print(all(pd.date_range(start=\"2015-01-01\",end=\"2020-12-31\").isin(price.index)))\n",
    "change_price=price.pct_change()\n",
    "\n",
    "print('change_price.index[:10]')\n",
    "print(change_price.index[:10])\n",
    "\n",
    "print('change_price.loc[change_price.index==2014-12-31].index:')\n",
    "print(change_price.loc[change_price.index=='2014-12-31'].index)\n",
    "\n",
    "## Eliminar periodos fuera del rango de analisis:\n",
    "change_price.drop(change_price.loc[change_price.index=='2014-12-31'].index, axis=0, inplace=True)\n",
    "\n",
    "#tweet_dataframe = tweet_dataframe.join(change_price, tweet_dataframe['post_date'] == change_price.index, 'left')\n",
    "\n",
    "# Export data to S3:\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "tweet_dyf = DynamicFrame.fromDF(tweet_dataframe, glueContext, \"tweet_parquet\")\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = tweet_dyf,\n",
    "       connection_type = \"s3\",\n",
    "       connection_options = {\"path\": \"s3://pi1-kjj/trusted/tweets\"},\n",
    "       format = \"json\")\n",
    "print('End storing to S3')\n",
    " \n",
    "###############CÓDIGO KARLA#############\n",
    "\n",
    "#IMPORTAR LIBRERIAS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "!pip install nltk\n",
    "\n",
    "#CARGA DE ARCHIVOS\n",
    "path_X1 = '/Users/karlaorozco/OneDrive_UniversidadEAFIT/Escritorio/INFORMACIÓN PERSONAL/1_MAESTRIA/SEMESTRE1/PI/Notebooks/Company_Tweet.csv'\n",
    "path_X2 = '/Users/karlaorozco/OneDrive_UniversidadEAFIT/Escritorio/INFORMACIÓN PERSONAL/1_MAESTRIA/SEMESTRE1/PI/Notebooks/Company1.csv'\n",
    "path_X3 = '/Users/karlaorozco/OneDrive_UniversidadEAFIT/Escritorio/INFORMACIÓN PERSONAL/1_MAESTRIA/SEMESTRE1/PI/Notebooks/Tweet.csv'\n",
    "path_X4 = '/Users/karlaorozco/OneDrive_UniversidadEAFIT/Escritorio/INFORMACIÓN PERSONAL/1_MAESTRIA/SEMESTRE1/PI/Notebooks/CompanyValues.csv'\n",
    "\n",
    "company_tweet = pd.read_csv(path_X1)\n",
    "company = pd.read_csv(path_X2)\n",
    "tweet = pd.read_csv(path_X3)\n",
    "company_values = pd.read_csv(path_X4)\n",
    "\n",
    "#CAMBIAR FECHA EN TABLA TWEET\n",
    "tweet['post_date'] = pd.to_datetime(tweet['post_date'],unit='s')\n",
    "\n",
    "#TOKENIZACIÓN DE 2000 TWEETS\n",
    "tweet_head = tweet.head(2000).copy()\n",
    "tweet_head['body_tokenized'] = tweet_head.apply(lambda row: nltk.word_tokenize(str(row['body'])), axis=1)\n",
    "tweet_head['body_tokenized']\n",
    "\n",
    "# STOPWORDS EN NLTK\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(f'Stopwords length: {len(stop_words_nltk)}')\n",
    "print(f'Stopwords: {stop_words_nltk}')\n",
    "\n",
    "#ELIMINACI´ÓN DE CARACTERES ESPECIALES \n",
    "\n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos\n",
    "# NUEVAMENTE ELIMINAR tokens de long = 1\n",
    "# REMOVER tokens conformados solo por numeros, ya que no necesitamos buscar expresiones solo numéricas\n",
    "# REMOVER stop words\n",
    "refined_tokens_by_file = []\n",
    "for (idx, tokens) in enumerate(tweet_head['body_tokenized']):\n",
    "  tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "  # tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
    "  tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "  for i, w in reversed(list(enumerate(tokens))):\n",
    "    try:\n",
    "      # Preguntamos si el token es solo numerico y si si, lo eliminamos\n",
    "      if (w.isnumeric()):\n",
    "        tokens.pop(i)\n",
    "    except:\n",
    "      pass\n",
    "  tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "  refined_tokens_by_file.append(tokens)\n",
    "  #fdist = nltk.FreqDist(refined_tokens_by_file)\n",
    "  # extract top 20 words\n",
    "  #topwords = fdist.most_common(20)\n",
    "  #print(f\"numero de palabras finales en {df['filename'][idx]} = {len(fdist)}\")\n",
    "  #print(f\"Top 20: {topwords}\")\n",
    "  # graficar los 20 términos más frecuentes:\n",
    "  #x,y = zip(*topwords)\n",
    "  #plt.figure(figsize=(15,10))\n",
    "  #plt.bar(x,y)\n",
    "  #plt.xlabel(\"Word\")\n",
    "  #plt.ylabel(\"frecuency\")\n",
    "  #plt.xticks(rotation=90)\n",
    "  #plt.show()\n",
    "  #print('------------------------------------------')\n",
    "tweet_head['body_tokenized'] = refined_tokens_by_file\n",
    "tweet_head.head()\n",
    "\n",
    "# LEMMA con NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens_by_file = []\n",
    "for (idx, tokens) in enumerate(refined_tokens_by_file):\n",
    "  lemmatized_tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
    "  lemmatized_tokens_by_file.append(lemmatized_tokens)\n",
    "  #fdist = nltk.FreqDist(lemmatized_tokens)\n",
    "  # extract top 20 words\n",
    "  #topwords = fdist.most_common(20)\n",
    "  #print(f\"numero de palabras finales en {tweet_head['body_tokenized'][idx]} = {len(fdist)}\")\n",
    "  #print(f\"Top 20: {topwords}\")\n",
    "  #x,y = zip(*topwords)\n",
    "  #plt.figure(figsize=(15,10))\n",
    "  #plt.bar(x,y)\n",
    "  #plt.xlabel(\"Word\")\n",
    "  #plt.ylabel(\"frecuency\")\n",
    "  #plt.xticks(rotation=90)\n",
    "  #plt.show()\n",
    "  #print('------------------------------------------')\n",
    "\n",
    "tweet_head['lemmatized_tokens'] = lemmatized_tokens_by_file\n",
    "tweet_head.head()\n",
    "\n",
    "#CREACIÓN DE COLUMNA LIST_LEMMA EN EL DF\n",
    "tweet_head['list_lemma'] = tweet_head['lemmatized_tokens'].apply(lambda c: list(w for w in c if not any(x.isdigit() for x in w)))\n",
    "\n",
    "#SE UNEN LOS TOKENS REFINADOS Y SE CREA UNA COLUMNA LLAMADA CLEAN_TWEET\n",
    "tweet_head['clean_tweet'] = tweet_head['list_lemma'].apply(lambda c: ' '.join(c))\n",
    "\n",
    "#ANÁLISIS DE SENTIMIENTOS\n",
    "#LIBRERIAS\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#SE CREA COLUMNA CON LOS SCORES DE LOS SENTIMIENTOS Y SE LLEVA A LA COLUMNA SENTIMENT\n",
    "tweet_head['score_vader'] = tweet_head['clean_tweet'].apply(lambda c: SentimentIntensityAnalyzer().polarity_scores(c)['compound'])\n",
    "\n",
    "#SE ASIGNA CONDICIONES PARA DEFINIR SI ES NEGATIVO, NEUTRAO O POSITIVO\n",
    "conditions = [\n",
    "    (tweet_head[\"score_vader\"] >= .05),\n",
    "    (tweet_head[\"score_vader\"] > -.05) & (tweet_head[\"score_vader\"] < .05),\n",
    "    (tweet_head[\"score_vader\"] <= -.05),\n",
    "]\n",
    "choices = ['positive', 'neutral', 'negative']\n",
    "tweet_head['sentiment'] = np.select(conditions, choices)\n",
    "tweet_head['sentiment'].value_counts()\n",
    "\n",
    "#TRANSFORMACI´ÓN DE SENTIMIENTOS EN ETIQUETAS [0,1,2]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "tweet_head['sentiment_code'] = le.fit_transform(tweet_head['sentiment'])\n",
    "np.unique(tweet_head['sentiment_code'])\n",
    "\n",
    "#ADICIONAR AL DF EMPRESAS DE LAS QUE HABLA CADA TWEET\n",
    "company_tweet=company_tweet.loc[company_tweet['tweet_id'].isin(tweet_head['tweet_id'])]\n",
    "ticker_symbol_group = company_tweet.groupby('tweet_id')['ticker_symbol'].unique()\n",
    "#Renombrar y mostrar la serie\n",
    "ticker_symbol_group.rename(\"ticker_symbol_group\", inplace=True)\n",
    "ticker_symbol_group.head(2)\n",
    "#Realizar combinación mencionada entre la tabla agrupada \"ticker_symbol_group\" y \"tweet\" por la columna \"tweet_id\":\n",
    "tweet_head = tweet_head.merge(ticker_symbol_group, how='left',left_on='tweet_id',right_on='tweet_id')\n",
    "\n",
    "#################### ****POR EMPRESA**** ########################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#SEPARAR CADA EMPRESA CON SUS SENTIMIENTOS Y HACER LOS EMBEDDINGS\n",
    "\n",
    "#********************* AAPL **************************\n",
    "df_aapl_idx = tweet_head.ticker_symbol_group.apply(lambda a: 'AAPL' in a)\n",
    "df_aapl= tweet_head[df_aapl_idx]\n",
    "\n",
    "#NEGATIVE = 0\n",
    "df_aapl_negative = df_aapl[df_aapl['sentiment_code']==0]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_aapl_negative_alltweets  = df_aapl_negative['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_aapl_negative = [df_aapl_negative_alltweets]\n",
    "vocabulary_aapl_negative = np.unique(df_aapl_negative_alltweets.split(\" \"))\n",
    "pipe_aapl_nega = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_aapl_negative)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_aapl_negative)\n",
    "pipe_aapl_nega['count'].transform(corpus_aapl_negative).toarray()\n",
    "pipe_aapl_nega['tfid'].idf_\n",
    "pipe_aapl_nega.transform(corpus_aapl_negative).shape\n",
    "\n",
    "#NEUTRAL = 1\n",
    "df_aapl_neutral = df_aapl[df_aapl['sentiment_code']==1]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_aapl_neutral_alltweets  = df_aapl_neutral['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_aapl_neutral = [df_aapl_neutral_alltweets]\n",
    "vocabulary_aapl_neutral = np.unique(df_aapl_neutral_alltweets.split(\" \"))\n",
    "pipe_aapl_neu = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_aapl_neutral)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_aapl_neutral)\n",
    "pipe_aapl_neu['count'].transform(corpus_aapl_neutral).toarray()\n",
    "pipe_aapl_neu['tfid'].idf_\n",
    "pipe_aapl_neu.transform(corpus_aapl_neutral).shape\n",
    "\n",
    "#POSITIVE = 2\n",
    "df_aapl_positive = df_aapl[df_aapl['sentiment_code']==2]\n",
    "### JOIN ALL TWEETS NEUTRAL\n",
    "df_aapl_positive_alltweets  = df_aapl_positive['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_aapl_positive = [df_aapl_positive_alltweets]\n",
    "vocabulary_aapl_positive = np.unique(df_aapl_positive_alltweets.split(\" \"))\n",
    "pipe_aapl_pos = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_aapl_positive)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_aapl_positive)\n",
    "pipe_aapl_pos['count'].transform(corpus_aapl_positive).toarray()\n",
    "pipe_aapl_pos['tfid'].idf_\n",
    "pipe_aapl_pos.transform(corpus_aapl_positive).shape\n",
    "\n",
    "#********************* TSLA **************************\n",
    "df_tsla_idx = tweet_head.ticker_symbol_group.apply(lambda a: 'TSLA' in a)\n",
    "df_tsla= tweet_head[df_tsla_idx]\n",
    "\n",
    "#NEGATIVE = 0\n",
    "df_tsla_negative = df_tsla[df_tsla['sentiment_code']==0]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_tsla_negative_alltweets  = df_tsla_negative['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_tsla_negative = [df_tsla_negative_alltweets]\n",
    "vocabulary_tsla_negative = np.unique(df_tsla_negative_alltweets.split(\" \"))\n",
    "pipe_tsla_nega = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_tsla_negative)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_tsla_negative)\n",
    "pipe_tsla_nega['count'].transform(corpus_tsla_negative).toarray()\n",
    "pipe_tsla_nega['tfid'].idf_\n",
    "pipe_tsla_nega.transform(corpus_tsla_negative).shape\n",
    "\n",
    "#NEUTRAL = 1\n",
    "df_tsla_neutral = df_tsla[df_tsla['sentiment_code']==1]\n",
    "### JOIN ALL TWEETS NEUTRAL\n",
    "df_tsla_neutral_alltweets  = df_tsla_neutral['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_tsla_neutral = [df_tsla_neutral_alltweets]\n",
    "vocabulary_tsla_neutral = np.unique(df_tsla_neutral_alltweets.split(\" \"))\n",
    "pipe_tsla_neu = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_tsla_neutral)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_tsla_neutral)\n",
    "pipe_tsla_neu['count'].transform(corpus_tsla_neutral).toarray()\n",
    "pipe_tsla_neu['tfid'].idf_\n",
    "pipe_tsla_neu.transform(corpus_tsla_neutral).shape\n",
    "\n",
    "#POSITIVE = 2\n",
    "df_tsla_positive = df_tsla[df_tsla['sentiment_code']==2]\n",
    "### JOIN ALL TWEETS POSITIVE\n",
    "df_tsla_positive_alltweets  = df_tsla_positive['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_tsla_pos = [df_tsla_positive_alltweets]\n",
    "vocabulary_tsla_pos = np.unique(df_tsla_positive_alltweets.split(\" \"))\n",
    "pipe_tsla_pos = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_tsla_pos)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_tsla_pos)\n",
    "pipe_tsla_pos['count'].transform(corpus_tsla_pos).toarray()\n",
    "pipe_tsla_pos['tfid'].idf_\n",
    "pipe_tsla_pos.transform(corpus_tsla_pos).shape\n",
    "\n",
    "#********************* AMZN **************************\n",
    "df_amzn_idx = tweet_head.ticker_symbol_group.apply(lambda a: 'AMZN' in a)\n",
    "df_amzn= tweet_head[df_amzn_idx]\n",
    "\n",
    "#NEGATIVE = 0\n",
    "df_amzn_negative = df_amzn[df_amzn['sentiment_code']==0]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_amzn_negative_alltweets  = df_amzn_negative['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_amzn_negative = [df_amzn_negative_alltweets]\n",
    "vocabulary_amzn_negative = np.unique(df_amzn_negative_alltweets.split(\" \"))\n",
    "pipe_amzn_nega = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_amzn_negative)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_amzn_negative)\n",
    "pipe_amzn_nega['count'].transform(corpus_amzn_negative).toarray()\n",
    "pipe_amzn_nega['tfid'].idf_\n",
    "pipe_amzn_nega.transform(corpus_amzn_negative).shape\n",
    "\n",
    "#NEUTRAL = 1\n",
    "df_amzn_neutral = df_amzn[df_amzn['sentiment_code']==1]\n",
    "### JOIN ALL TWEETS NEUTRAL\n",
    "df_amzn_neutral_alltweets  = df_amzn_neutral['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_amzn_neutral = [df_amzn_neutral_alltweets]\n",
    "vocabulary_amzn_neutral = np.unique(df_amzn_neutral_alltweets.split(\" \"))\n",
    "pipe_amzn_neu = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_amzn_neutral)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_amzn_neutral)\n",
    "pipe_amzn_neu['count'].transform(corpus_amzn_neutral).toarray()\n",
    "pipe_amzn_neu['tfid'].idf_\n",
    "pipe_amzn_neu.transform(corpus_amzn_neutral).shape\n",
    "\n",
    "#POSITIVE = 2\n",
    "df_amzn_positive = df_amzn[df_amzn['sentiment_code']==2]\n",
    "### JOIN ALL TWEETS POSITIVE\n",
    "df_amzn_positive_alltweets  = df_amzn_positive['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_amzn_pos = [df_amzn_positive_alltweets]\n",
    "vocabulary_amzn_pos = np.unique(df_amzn_positive_alltweets.split(\" \"))\n",
    "pipe_amzn_pos = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_amzn_pos)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_amzn_pos)\n",
    "pipe_amzn_pos['count'].transform(corpus_amzn_pos).toarray()\n",
    "pipe_amzn_pos['tfid'].idf_\n",
    "pipe_amzn_pos.transform(corpus_amzn_pos).shape\n",
    "\n",
    "#********************* GOOG **************************\n",
    "df_goog_idx = tweet_head.ticker_symbol_group.apply(lambda a: 'GOOG' in a)\n",
    "df_goog= tweet_head[df_goog_idx]\n",
    "\n",
    "#NEGATIVE = 0\n",
    "df_goog_negative = df_goog[df_goog['sentiment_code']==0]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_goog_negative_alltweets  = df_goog_negative['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_goog_negative = [df_goog_negative_alltweets]\n",
    "vocabulary_goog_negative = np.unique(df_goog_negative_alltweets.split(\" \"))\n",
    "pipe_goog_nega = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_goog_negative)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_goog_negative)\n",
    "pipe_goog_nega['count'].transform(corpus_goog_negative).toarray()\n",
    "pipe_goog_nega['tfid'].idf_\n",
    "pipe_goog_nega.transform(corpus_goog_negative).shape\n",
    "\n",
    "#NEUTRAL = \n",
    "df_goog_neutral = df_goog[df_goog['sentiment_code']==1]\n",
    "### JOIN ALL TWEETS NEUTRAL\n",
    "df_goog_neutral_alltweets  = df_goog_neutral['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_goog_neutral = [df_goog_neutral_alltweets]\n",
    "vocabulary_goog_neutral = np.unique(df_goog_neutral_alltweets.split(\" \"))\n",
    "pipe_goog_neu = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_goog_neutral)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_goog_neutral)\n",
    "pipe_goog_neu['count'].transform(corpus_goog_neutral).toarray()\n",
    "pipe_goog_neu['tfid'].idf_\n",
    "pipe_goog_neu.transform(corpus_goog_neutral).shape\n",
    "\n",
    "#POSITIVE = 2\n",
    "df_goog_positive = df_goog[df_goog['sentiment_code']==2]\n",
    "### JOIN ALL TWEETS POSITIVE\n",
    "df_goog_positive_alltweets  = df_goog_positive['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_goog_pos = [df_goog_positive_alltweets]\n",
    "vocabulary_goog_pos = np.unique(df_goog_positive_alltweets.split(\" \"))\n",
    "pipe_goog_pos = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_goog_pos)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_goog_pos)\n",
    "pipe_goog_pos['count'].transform(corpus_goog_pos).toarray()\n",
    "pipe_goog_pos['tfid'].idf_\n",
    "pipe_goog_pos.transform(corpus_goog_pos).shape\n",
    "\n",
    "#********************* GOOGL **************************\n",
    "df_googl_idx = tweet_head.ticker_symbol_group.apply(lambda a: 'GOOGL' in a)\n",
    "df_googl= tweet_head[df_googl_idx]\n",
    "\n",
    "#NEGATIVE = 0\n",
    "df_googl_negative = df_googl[df_googl['sentiment_code']==0]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_googl_negative_alltweets  = df_googl_negative['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_googl_negative = [df_googl_negative_alltweets]\n",
    "vocabulary_googl_negative = np.unique(df_googl_negative_alltweets.split(\" \"))\n",
    "pipe_googl_nega = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_googl_negative)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_googl_negative)\n",
    "pipe_googl_nega['count'].transform(corpus_googl_negative).toarray()\n",
    "pipe_googl_nega['tfid'].idf_\n",
    "pipe_googl_nega.transform(corpus_googl_negative).shape\n",
    "\n",
    "#NEUTRAL = 1\n",
    "df_googl_neutral = df_googl[df_googl['sentiment_code']==1]\n",
    "### JOIN ALL TWEETS NEUTRAL\n",
    "df_googl_neutral_alltweets  = df_googl_neutral['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_googl_neutral = [df_googl_neutral_alltweets]\n",
    "vocabulary_googl_neutral = np.unique(df_googl_neutral_alltweets.split(\" \"))\n",
    "pipe_googl_neu = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_googl_neutral)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_googl_neutral)\n",
    "pipe_googl_neu['count'].transform(corpus_googl_neutral).toarray()\n",
    "pipe_googl_neu['tfid'].idf_\n",
    "pipe_googl_neu.transform(corpus_googl_neutral).shape\n",
    "\n",
    "#POSITIVE = 2\n",
    "df_googl_positive = df_googl[df_googl['sentiment_code']==2]\n",
    "### JOIN ALL TWEETS POSITIVE\n",
    "df_googl_positive_alltweets  = df_googl_positive['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_googl_pos = [df_googl_positive_alltweets]\n",
    "vocabulary_googl_pos = np.unique(df_googl_positive_alltweets.split(\" \"))\n",
    "pipe_googl_pos = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_googl_pos)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_googl_pos)\n",
    "pipe_googl_pos['count'].transform(corpus_googl_pos).toarray()\n",
    "pipe_googl_pos['tfid'].idf_\n",
    "pipe_googl_pos.transform(corpus_googl_pos).shape\n",
    "\n",
    "#********************* MSFT **************************\n",
    "df_msft_idx = tweet_head.ticker_symbol_group.apply(lambda a: 'MSFT' in a)\n",
    "df_msft = tweet_head[df_msft_idx]\n",
    "\n",
    "#NEGATIVE = 0\n",
    "df_msft_negative = df_msft[df_msft['sentiment_code']==0]\n",
    "### JOIN ALL TWEETS NEGATIVE\n",
    "df_msft_negative_alltweets  = df_msft_negative['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_msft_negative = [df_msft_negative_alltweets]\n",
    "vocabulary_msft_negative = np.unique(df_msft_negative_alltweets.split(\" \"))\n",
    "pipe_msft_nega = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_msft_negative)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_msft_negative)\n",
    "pipe_msft_nega['count'].transform(corpus_msft_negative).toarray()\n",
    "pipe_msft_nega['tfid'].idf_\n",
    "pipe_msft_nega.transform(corpus_msft_negative).shape\n",
    "\n",
    "#NEUTRAL = 1\n",
    "df_msft_neutral = df_msft[df_msft['sentiment_code']==1]\n",
    "### JOIN ALL TWEETS NEUTRAL\n",
    "df_msft_neutral_alltweets  = df_msft_neutral['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_msft_neutral = [df_msft_neutral_alltweets]\n",
    "vocabulary_msft_neutral = np.unique(df_msft_neutral_alltweets.split(\" \"))\n",
    "pipe_msft_neu = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_msft_neutral)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_msft_neutral)\n",
    "pipe_msft_neu['count'].transform(corpus_msft_neutral).toarray()\n",
    "pipe_msft_neu['tfid'].idf_\n",
    "pipe_msft_neu.transform(corpus_msft_neutral).shape\n",
    "\n",
    "#POSITIVE = 2\n",
    "df_msft_positive = df_msft[df_msft['sentiment_code']==2]\n",
    "### JOIN ALL TWEETS POSITIVE\n",
    "df_msft_positive_alltweets  = df_msft_positive['clean_tweet'].str.cat(sep = ' ')\n",
    "#EMBEDDINGS\n",
    "corpus_msft_pos = [df_msft_positive_alltweets]\n",
    "vocabulary_msft_pos = np.unique(df_msft_positive_alltweets.split(\" \"))\n",
    "pipe_msft_pos = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_msft_pos)),\n",
    "                  ('tfid', TfidfTransformer())]).fit(corpus_msft_pos)\n",
    "pipe_msft_pos['count'].transform(corpus_msft_pos).toarray()\n",
    "pipe_msft_pos['tfid'].idf_\n",
    "pipe_msft_pos.transform(corpus_msft_pos).shape\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
