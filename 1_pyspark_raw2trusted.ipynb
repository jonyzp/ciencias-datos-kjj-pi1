{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Integrador 1\n",
    "---\n",
    "Presentado por:\n",
    "* Karla Orozco\n",
    "* Jonathan zapata\n",
    "* Juan Fernando Gallego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw -> Trusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries:\n",
    "import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/04 20:11:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/04 20:11:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## @params: [JOB_NAME]\n",
    "# args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)\n",
    "# job.init(args['JOB_NAME'], args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Glue Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ticker_symbol: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: long (nullable = true)\n",
      " |-- ticker_symbol: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: long (nullable = true)\n",
      " |-- writer: string (nullable = true)\n",
      " |-- post_date: integer (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- comment_num: string (nullable = true)\n",
      " |-- retweet_num: string (nullable = true)\n",
      " |-- like_num: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "## @type: DataSource\n",
    "## @args: [database = \"pi1-kjj-trusted\", table_name = \"company1_csv\", transformation_ctx = \"DataSource0\"]\n",
    "## @return: company_dynamicframe\n",
    "## @inputs: []\n",
    "# company_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "#        database = \"pi1-kjj-trusted\",\n",
    "#        table_name = \"company_csv\",\n",
    "#        transformation_ctx = \"company_dynamicframe\")\n",
    "company_dynamicframe = spark.read.csv('raw/Company.csv', header='true', inferSchema='true')\n",
    "company_dynamicframe.printSchema()\n",
    "\n",
    "# company_tweet_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "#        database = \"pi1-kjj-trusted\",\n",
    "#        table_name = \"company_tweet_csv\")\n",
    "company_tweet_dynamicframe = spark.read.csv('raw/Company_Tweet.csv', header='true', inferSchema='true')\n",
    "company_tweet_dynamicframe.printSchema()\n",
    "\n",
    "# tweet_dynamicframe = glueContext.create_dynamic_frame.from_catalog(\n",
    "#        database = \"pi1-kjj-trusted\",\n",
    "#        table_name = \"tweet_csv\")\n",
    "tweet_dynamicframe = spark.read.csv('raw/Tweet.csv', header='true', inferSchema='true')\n",
    "tweet_dynamicframe.printSchema()\n",
    "\n",
    "# company_dataframe = company_dynamicframe.toDF()\n",
    "# tweet_dataframe = tweet_dynamicframe.toDF()\n",
    "# company_tweet_dataframe = company_tweet_dynamicframe.toDF()\n",
    "\n",
    "company_dataframe = company_dynamicframe\n",
    "tweet_dataframe = tweet_dynamicframe\n",
    "company_tweet_dataframe = company_tweet_dynamicframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize total engagement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[Row(tweet_id=550441509175443456, writer='VisualStockRSRC', post_datetime_epoch=1420070457, body='lx21 made $10,008  on $AAPL -Check it out! http://profit.ly/1MnD8s?aff=202 Learn #howtotrade http://bit.ly/1c1NljX $EXE $WATT $IMRS $CACH $GMO', comment_num='0', retweet_num='0', like_num='1', total_engagement=1.0),\n",
       " Row(tweet_id=550441672312512512, writer='KeralaGuy77', post_datetime_epoch=1420070496, body='Insanity of today weirdo massive selling. $aapl bid up 45 cents after hours after non stop selling in trading hours', comment_num='0', retweet_num='0', like_num='0', total_engagement=0.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Summarize total engagement:\n",
    "# tweet_dataframe['total_engagement'] = tweet_dataframe['comment_num'] + tweet_dataframe['retweet_num'] + tweet_dataframe['like_num']\n",
    "tweet_dataframe = tweet_dataframe.withColumn('total_engagement', sum(tweet_dataframe[col] for col in ['comment_num', 'retweet_num', 'like_num']))\n",
    "\n",
    "# Get iso date (Not epoch)\n",
    "tweet_dataframe = tweet_dataframe.withColumnRenamed('post_date', 'post_datetime_epoch')\n",
    "# tweet_dataframe['post_datetime'] = pd.to_datetime(tweet_dataframe['post_datetime_epoch'],unit='s')\n",
    "tweet_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get only date from datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as f\n",
    "# https://stackoverflow.com/a/54340652\n",
    "tweet_dataframe = tweet_dataframe.withColumn('post_datetime', f.to_timestamp(tweet_dataframe.post_datetime_epoch))\n",
    "# tweet_dataframe['post_date'] = tweet_dataframe['post_datetime'].dt.date\n",
    "tweet_dataframe = tweet_dataframe.withColumn('post_date', f.to_date(tweet_dataframe.post_datetime))\n",
    "#tweet_dataframe = tweet_dataframe.withColumn('post_date', f.to_date(f.col('post_datetime')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ticker symbol for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join company tweets with tweets dataframe:\n",
    "# ticker_symbol_group = company_tweet_dataframe.groupBy('tweet_id')['ticker_symbol'].distinct()\n",
    "# ticker_symbol_group.rename(\"ticker_symbol_group\", inplace=True)\n",
    "ticker_symbol_group = company_tweet_dataframe.groupBy('tweet_id').agg(\n",
    "    f.collect_set(f.col('ticker_symbol')).alias('ticker_symbol_group')\n",
    ")\n",
    "# print(ticker_symbol_group.show(n=5))\n",
    "\n",
    "tweet_dataframe = tweet_dataframe.join(ticker_symbol_group, tweet_dataframe['tweet_id'] == ticker_symbol_group['tweet_id'], 'left')\n",
    "# print(tweet_dataframe.show(truncate=False, n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize & Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/04 20:12:08 WARN StopWordsRemover: Default locale set was [es_419]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "#tweet_head['body_tokenized'] = tweet_head.apply(lambda row: nltk.word_tokenize(str(row['body'])), axis=1)\n",
    "#tweet_dataframe = tweet_dataframe.withColumn('body_tokenized', nltk.word_tokenize(str(col)) for col in ['body'])\n",
    "# tweet_dataframe = tweet_dataframe.withColumn('body_tokenized', tweet_dataframe.select('body').rdd.map(lambda x: \n",
    "#     TweetTokenizer(str(x))).collect()\n",
    "# )\n",
    "# print(tweet_dataframe.show(n=5))\n",
    "\n",
    "# stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "# https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"body_tokenized\")\n",
    "remove_stop_words = StopWordsRemover(inputCol=\"body_tokenized\", outputCol=\"no_stopwords\")\n",
    "unigrammer = NGram(n=1, inputCol=\"no_stopwords\", outputCol=\"ngrams\") \n",
    "\n",
    "\n",
    "data_prep_pipe = Pipeline(\n",
    "    stages=[tokenizer, remove_stop_words, unigrammer]\n",
    ")\n",
    "\n",
    "\n",
    "transformed = data_prep_pipe.fit(tweet_dataframe).transform(tweet_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: long (nullable = true)\n",
      " |-- writer: string (nullable = true)\n",
      " |-- post_datetime_epoch: integer (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- comment_num: string (nullable = true)\n",
      " |-- retweet_num: string (nullable = true)\n",
      " |-- like_num: string (nullable = true)\n",
      " |-- total_engagement: double (nullable = true)\n",
      " |-- post_datetime: timestamp (nullable = true)\n",
      " |-- post_date: date (nullable = true)\n",
      " |-- tweet_id: long (nullable = true)\n",
      " |-- ticker_symbol_group: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- body_tokenized: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- no_stopwords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transformed.select(['body', 'ngrams']).show()\n",
    "\n",
    "transformed.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 bow|\n",
      "+--------------------+\n",
      "|lx21 made $10,008...|\n",
      "|insanity today we...|\n",
      "|s&p100 #stocks pe...|\n",
      "|$gm $tsla: volksw...|\n",
      "|swing trading: 8....|\n",
      "|swing trading: 8....|\n",
      "|swing trading: 8....|\n",
      "|swing trading: 8....|\n",
      "|swing trading: 8....|\n",
      "|swing trading: 8....|\n",
      "|swing trading: 8....|\n",
      "|$unp $orcl $qcom ...|\n",
      "|$aapl apple goes ...|\n",
      "|“@wsj: apple sued...|\n",
      "|apple filed stylu...|\n",
      "|@cnbc 15 top #tra...|\n",
      "|searched hundreds...|\n",
      "|top 10 searched #...|\n",
      "|2014 year review ...|\n",
      "|give brain workou...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "transformed = transformed.withColumn('bow', concat_ws(' ', 'ngrams'))\n",
    "\n",
    "transformed.select('bow').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jonny/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/04 18:36:15 WARN SimpleFunctionRegistry: The function sentiment replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@udf\n",
    "def sentiment(x):\n",
    "  import os\n",
    "  os.environ['NLTK_DATA'] = '/tmp'\n",
    "  import nltk\n",
    "  nltk.data.path.append('/tmp')\n",
    "  print('nltk.data.path')\n",
    "  print(nltk.data.path)\n",
    "  # nltk.downloader.download('vader_lexicon')\n",
    "  nltk.download('vader_lexicon', download_dir = '/tmp') # For Sentyment Analysis\n",
    "  return SentimentIntensityAnalyzer().polarity_scores(x)['compound']\n",
    "\n",
    "# sentiment = udf(lambda x: SentimentIntensityAnalyzer().polarity_scores(x)['compound'])\n",
    "# spark.udf.register('sentiment', sentiment)\n",
    "transformed = transformed.withColumn('score_vader', sentiment('bow').cast('double'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.select('score_vader').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conditions = [\n",
    "#     (tweet[\"score_vader\"] >= .05),\n",
    "#     (tweet[\"score_vader\"] > -.05) & (tweet[\"score_vader\"] < .05),\n",
    "#     (tweet[\"score_vader\"] <= -.05),\n",
    "# ]\n",
    "\n",
    "# choices = ['positive', 'neutral', 'negative']\n",
    "# tweet['sentiment'] = np.select(conditions, choices)\n",
    "\n",
    "@udf\n",
    "def scorevader_classifier(s):\n",
    "  if (s >= 0.05): return 'positive'\n",
    "  elif s <= -0.05: return 'neutral'\n",
    "  else: return 'negative'\n",
    "\n",
    "transformed = transformed.withColumn('sentiment', scorevader_classifier('score_vader'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by ticker symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|ticker_symbol|company_name|\n",
      "+-------------+------------+\n",
      "|         AAPL|       apple|\n",
      "|         GOOG|  Google Inc|\n",
      "|        GOOGL|  Google Inc|\n",
      "|         AMZN|  Amazon.com|\n",
      "|         TSLA|   Tesla Inc|\n",
      "|         MSFT|   Microsoft|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# company_dataframe.show(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-------------------+--------------------+-----------+-----------+--------+----------------+-------------------+----------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_id|         writer|post_datetime_epoch|                body|comment_num|retweet_num|like_num|total_engagement|      post_datetime| post_date|          tweet_id|ticker_symbol_group|      body_tokenized|        no_stopwords|              ngrams|                 bow|\n",
      "+------------------+---------------+-------------------+--------------------+-----------+-----------+--------+----------------+-------------------+----------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|550447574285418497|      btcgemini|         1420071903|We searched throu...|          0|          0|       0|             0.0|2014-12-31 19:25:03|2014-12-31|550447574285418497|             [AAPL]|[we, searched, th...|[searched, hundre...|[searched, hundre...|searched hundreds...|\n",
      "|550450683309072384|     10Xtrading|         1420072644|Clay Trader Apple...|          0|          0|       0|             0.0|2014-12-31 19:37:24|2014-12-31|550450683309072384|             [AAPL]|[clay, trader, ap...|[clay, trader, ap...|[clay, trader, ap...|clay trader apple...|\n",
      "|550489113468866561| InTheKnowMoney|         1420081806|The week's winner...|          0|          1|       0|             1.0|2014-12-31 22:10:06|2014-12-31|550489113468866561|             [AAPL]|[the, week's, win...|[week's, winners,...|[week's, winners,...|week's winners lo...|\n",
      "|550502942407929857|       smoran26|         1420085103|Yes to the first ...|          0|          0|       0|             0.0|2014-12-31 23:05:03|2014-12-31|550502942407929857|             [AAPL]|[yes, to, the, fi...|[yes, first, pers...|[yes, first, pers...|yes first person ...|\n",
      "|550503223485009920|       smoran26|         1420085170|If you had a fant...|          0|          0|       0|             0.0|2014-12-31 23:06:10|2014-12-31|550503223485009920|             [AAPL]|[if, you, had, a,...|[fantasy, blue, -...|[fantasy, blue, -...|fantasy blue - ch...|\n",
      "|550505446134386689|       smoran26|         1420085700|Stock of the Year...|          0|          0|       0|             0.0|2014-12-31 23:15:00|2014-12-31|550505446134386689|             [AAPL]|[stock, of, the, ...|[stock, year:, $a...|[stock, year:, $a...|stock year: $aapl...|\n",
      "|550514037155704832|     davidspitz|         1420087749|@skupor @business...|          0|          0|       1|             1.0|2014-12-31 23:49:09|2014-12-31|550514037155704832|             [AAPL]|[@skupor, @busine...|[@skupor, @busine...|[@skupor, @busine...|@skupor @business...|\n",
      "|550521659044552706|bullandbearmash|         1420089566|\"Commented on: \"\"...|          0|          0|       0|             0.0|2015-01-01 00:19:26|2015-01-01|550521659044552706|             [AAPL]|[\"commented, on:,...|[\"commented, on:,...|[\"commented, on:,...|\"commented on: \"\"...|\n",
      "|550530027012382721|       LeRatton|         1420091561|Apple: Asian Carr...|          0|          0|       0|             0.0|2015-01-01 00:52:41|2015-01-01|550530027012382721|       [GOOG, AAPL]|[apple:, asian, c...|[apple:, asian, c...|[apple:, asian, c...|apple: asian carr...|\n",
      "|550543810468773888|       FTUtweet|         1420094847|$AAPL stock conte...|          0|          0|       0|             0.0|2015-01-01 01:47:27|2015-01-01|550543810468773888|             [AAPL]|[$aapl, stock, co...|[$aapl, stock, co...|[$aapl, stock, co...|$aapl stock conte...|\n",
      "|550590005878329344|   SeekingAlpha|         1420105861|Apple, Micron, Su...|          0|          2|       1|             3.0|2015-01-01 04:51:01|2015-01-01|550590005878329344|             [AAPL]|[apple,, micron,,...|[apple,, micron,,...|[apple,, micron,,...|apple, micron, su...|\n",
      "|550602865690288128|      Sara20992|         1420108927|THE BINARY OPTION...|          0|          0|       0|             0.0|2015-01-01 05:42:07|2015-01-01|550602865690288128|      [GOOGL, AAPL]|[the, binary, opt...|[binary, options,...|[binary, options,...|binary options co...|\n",
      "|550622416213651456|     SentiQuant|         1420113588|#VOLUMESHIFTUP $I...|          0|          0|       0|             0.0|2015-01-01 06:59:48|2015-01-01|550622416213651456|             [AAPL]|[#volumeshiftup, ...|[#volumeshiftup, ...|[#volumeshiftup, ...|#volumeshiftup $i...|\n",
      "|550631853049122817|     sportadore|         1420115838|Best Dividend Sto...|          0|          0|       0|             0.0|2015-01-01 07:37:18|2015-01-01|550631853049122817|             [AAPL]|[best, dividend, ...|[best, dividend, ...|[best, dividend, ...|best dividend sto...|\n",
      "|550653576716890113|LloydCreekStock|         1420121017|2015 Will Be Appl...|          0|          0|       1|             1.0|2015-01-01 09:03:37|2015-01-01|550653576716890113|             [AAPL]|[2015, will, be, ...|[2015, apple's, y...|[2015, apple's, y...|2015 apple's year...|\n",
      "|550678646868828160|laurenholmesNYC|         1420126995|$FB $AAPL $YHOO $...|          1|          0|       1|             2.0|2015-01-01 10:43:15|2015-01-01|550678646868828160|             [AAPL]|[$fb, $aapl, $yho...|[$fb, $aapl, $yho...|[$fb, $aapl, $yho...|$fb $aapl $yhoo $...|\n",
      "|550690188066713601|laurenholmesNYC|         1420129746|What happened to ...|          0|          0|       2|             2.0|2015-01-01 11:29:06|2015-01-01|550690188066713601| [GOOG, AAPL, MSFT]|[what, happened, ...|[happened, $ibm?,...|[happened, $ibm?,...|happened $ibm? ==...|\n",
      "|550701004040073216|    MacHashNews|         1420132325|Become a personal...|          0|          0|       2|             2.0|2015-01-01 12:12:05|2015-01-01|550701004040073216|             [AAPL]|[become, a, perso...|[become, personal...|[become, personal...|become personal f...|\n",
      "|550705849690116096|SwingTradeAlert|         1420133480|IBD:2015 technolo...|          0|          0|       3|             3.0|2015-01-01 12:31:20|2015-01-01|550705849690116096| [AMZN, AAPL, MSFT]|[ibd:2015, techno...|[ibd:2015, techno...|[ibd:2015, techno...|ibd:2015 technolo...|\n",
      "|550708803922980865|    MacHashNews|         1420134185|Review: iOgrapher...|          0|          0|       1|             1.0|2015-01-01 12:43:05|2015-01-01|550708803922980865|             [AAPL]|[review:, iograph...|[review:, iograph...|[review:, iograph...|review: iographer...|\n",
      "+------------------+---------------+-------------------+--------------------+-----------+-----------+--------+----------------+-------------------+----------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# def split_data(x, v_split):\n",
    "#     #columna de listas de tickers\n",
    "#     t=x['ticker_symbol_group']\n",
    "#     #vector de boleanos\n",
    "#     v_bool=t.apply(lambda tikers: np.isin(v_split,tikers))\n",
    "#     #retornar datos filtrados\n",
    "#     return x.loc[v_bool].copy()\n",
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "AAPL = transformed \\\n",
    "  .filter(array_contains(col('ticker_symbol_group'), 'AAPL'))\n",
    "\n",
    "AAPL.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-------------------+--------------------+-----------+-----------+--------+----------------+-------------------+----------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_id|         writer|post_datetime_epoch|                body|comment_num|retweet_num|like_num|total_engagement|      post_datetime| post_date|          tweet_id|ticker_symbol_group|      body_tokenized|        no_stopwords|              ngrams|                 bow|\n",
      "+------------------+---------------+-------------------+--------------------+-----------+-----------+--------+----------------+-------------------+----------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|550461555423584257|     t_nathan95|         1420075236|Prediction: $TWTR...|          0|          0|       1|             1.0|2014-12-31 20:20:36|2014-12-31|550461555423584257|      [GOOGL, GOOG]|[prediction:, $tw...|[prediction:, $tw...|[prediction:, $tw...|prediction: $twtr...|\n",
      "|550530027012382721|       LeRatton|         1420091561|Apple: Asian Carr...|          0|          0|       0|             0.0|2015-01-01 00:52:41|2015-01-01|550530027012382721|       [GOOG, AAPL]|[apple:, asian, c...|[apple:, asian, c...|[apple:, asian, c...|apple: asian carr...|\n",
      "|550541216262402048|       ADVFNplc|         1420094229|$GOOG - Music Dow...|          0|          0|       0|             0.0|2015-01-01 01:37:09|2015-01-01|550541216262402048|             [GOOG]|[$goog, -, music,...|[$goog, -, music,...|[$goog, -, music,...|$goog - music dow...|\n",
      "|550597922639720448|   SeekingAlpha|         1420107749|2015 Outlook And ...|          0|          0|       0|             0.0|2015-01-01 05:22:29|2015-01-01|550597922639720448|             [GOOG]|[2015, outlook, a...|[2015, outlook, p...|[2015, outlook, p...|2015 outlook pick...|\n",
      "|550636326308282368|   Stockaholics|         1420116905|Our Penny Stock P...|          0|          0|       0|             0.0|2015-01-01 07:55:05|2015-01-01|550636326308282368|             [GOOG]|[our, penny, stoc...|[penny, stock, pi...|[penny, stock, pi...|penny stock picks...|\n",
      "|550689740169564160|laurenholmesNYC|         1420129640|$GOOG - Founder w...|          0|          0|       0|             0.0|2015-01-01 11:27:20|2015-01-01|550689740169564160|             [GOOG]|[$goog, -, founde...|[$goog, -, founde...|[$goog, -, founde...|$goog - founder m...|\n",
      "|550690188066713601|laurenholmesNYC|         1420129746|What happened to ...|          0|          0|       2|             2.0|2015-01-01 11:29:06|2015-01-01|550690188066713601| [GOOG, AAPL, MSFT]|[what, happened, ...|[happened, $ibm?,...|[happened, $ibm?,...|happened $ibm? ==...|\n",
      "|550703285343387649|Market_Screener|         1420132869|#GOOGLE 'C' : Can...|          0|          0|       0|             0.0|2015-01-01 12:21:09|2015-01-01|550703285343387649|             [GOOG]|[#google, 'c', :,...|[#google, 'c', :,...|[#google, 'c', :,...|#google 'c' : can...|\n",
      "|550709117439197185|    HotpageNews|         1420134259|#Google Fiber’s l...|          0|          0|       0|             0.0|2015-01-01 12:44:19|2015-01-01|550709117439197185|             [GOOG]|[#google, fiber’s...|[#google, fiber’s...|[#google, fiber’s...|#google fiber’s l...|\n",
      "|550762894624051200| Stock_Tracker1|         1420147081|Google Inc Class ...|          0|          0|       0|             0.0|2015-01-01 16:18:01|2015-01-01|550762894624051200|             [GOOG]|[google, inc, cla...|[google, inc, cla...|[google, inc, cla...|google inc class ...|\n",
      "|550808614647369729|         TMFJMo|         1420157981|More competition ...|          0|          0|       1|             1.0|2015-01-01 19:19:41|2015-01-01|550808614647369729|             [GOOG]|[more, competitio...|[competition, ad,...|[competition, ad,...|competition ad do...|\n",
      "|550999969180549120|     SAlphaAAPL|         1420203604|Apple: Asian Carr...|          0|          0|       0|             0.0|2015-01-02 08:00:04|2015-01-02|550999969180549120|[GOOGL, GOOG, AAPL]|[apple:, asian, c...|[apple:, asian, c...|[apple:, asian, c...|apple: asian carr...|\n",
      "|551010745463234560|  GiblioniChris|         1420206173|$AAL $FB $BABA $G...|          0|          0|       0|             0.0|2015-01-02 08:42:53|2015-01-02|551010745463234560|             [GOOG]|[$aal, $fb, $baba...|[$aal, $fb, $baba...|[$aal, $fb, $baba...|$aal $fb $baba $g...|\n",
      "|551010796722212864| totalinvestor1|         1420206185|5 Massive Upside ...|          0|          0|       0|             0.0|2015-01-02 08:43:05|2015-01-02|551010796722212864|             [GOOG]|[5, massive, upsi...|[5, massive, upsi...|[5, massive, upsi...|5 massive upside ...|\n",
      "|551032000488820740|     trohd40054|         1420211241|$GOOG this is loo...|          0|          0|       0|             0.0|2015-01-02 10:07:21|2015-01-02|551032000488820740|             [GOOG]|[$goog, this, is,...|[$goog, looking, ...|[$goog, looking, ...|$goog looking gre...|\n",
      "|551039726866558978|FxTradingUnlock|         1420213083|$GOOG is in a Bul...|          0|          0|       0|             0.0|2015-01-02 10:38:03|2015-01-02|551039726866558978|             [GOOG]|[$goog, is, in, a...|[$goog, bullish, ...|[$goog, bullish, ...|$goog bullish pos...|\n",
      "|551060093144268801|  PipsToDollars|         1420217939|CES 2015: what to...|          0|          0|       0|             0.0|2015-01-02 11:58:59|2015-01-02|551060093144268801|             [GOOG]|[ces, 2015:, what...|[ces, 2015:, expe...|[ces, 2015:, expe...|ces 2015: expect ...|\n",
      "|551069692233015296|     AlyceLomax|         1420220227|Randomly remember...|          1|          0|       0|             1.0|2015-01-02 12:37:07|2015-01-02|551069692233015296|             [GOOG]|[randomly, rememb...|[randomly, rememb...|[randomly, rememb...|randomly remember...|\n",
      "|551070095905423362|        CBOE_TV|         1420220323|$AAPL @ the$ acti...|          0|          0|       0|             0.0|2015-01-02 12:38:43|2015-01-02|551070095905423362|       [GOOG, AAPL]|[$aapl, @, the$, ...|[$aapl, @, the$, ...|[$aapl, @, the$, ...|$aapl @ the$ acti...|\n",
      "|551092881419939840|   Stockaholics|         1420225756|Our Penny Stock P...|          0|          0|       0|             0.0|2015-01-02 14:09:16|2015-01-02|551092881419939840|             [GOOG]|[our, penny, stoc...|[penny, stock, pi...|[penny, stock, pi...|penny stock picks...|\n",
      "+------------------+---------------+-------------------+--------------------+-----------+-----------+--------+----------------+-------------------+----------+------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "GOOG = transformed \\\n",
    "  .filter(array_contains(col('ticker_symbol_group'), 'GOOG'))\n",
    "\n",
    "GOOG.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GOOGL = transformed \\\n",
    "  .filter(array_contains(col('ticker_symbol_group'), 'GOOGL'))\n",
    "\n",
    "GOOGL.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AMZN = transformed \\\n",
    "  .filter(array_contains(col('ticker_symbol_group'), 'AMZN'))\n",
    "\n",
    "AMZN.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TSLA = transformed \\\n",
    "  .filter(array_contains(col('ticker_symbol_group'), 'TSLA'))\n",
    "\n",
    "TSLA.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MSFT = transformed \\\n",
    "  .filter(array_contains(col('ticker_symbol_group'), 'MSFT'))\n",
    "\n",
    "MSFT.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to S3:\n",
    "\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "AAPL_dyf = DynamicFrame.fromDF(AAPL, glueContext, 'AAPL')\n",
    "GOOG_dyf = DynamicFrame.fromDF(GOOG, glueContext, 'GOOG')\n",
    "GOOGL_dyf = DynamicFrame.fromDF(GOOGL, glueContext, 'GOOGL')\n",
    "AMZN_dyf = DynamicFrame.fromDF(AMZN, glueContext, 'AMZN')\n",
    "TSLA_dyf = DynamicFrame.fromDF(TSLA, glueContext, 'TSLA')\n",
    "MSFT_dyf = DynamicFrame.fromDF(MSFT, glueContext, 'MSFT')\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = AAPL_dyf,\n",
    "       connection_type = 's3',\n",
    "       connection_options = {'path': 's3://pi1-kjj/trusted/AAPL'},\n",
    "       format = 'parquet')\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = GOOG_dyf,\n",
    "       connection_type = 's3',\n",
    "       connection_options = {'path': 's3://pi1-kjj/trusted/GOOG'},\n",
    "       format = 'parquet')\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = GOOGL_dyf,\n",
    "       connection_type = 's3',\n",
    "       connection_options = {'path': 's3://pi1-kjj/trusted/GOOGL'},\n",
    "       format = 'parquet')\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = AMZN_dyf,\n",
    "       connection_type = 's3',\n",
    "       connection_options = {'path': 's3://pi1-kjj/trusted/AMZN'},\n",
    "       format = 'parquet')\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = TSLA_dyf,\n",
    "       connection_type = 's3',\n",
    "       connection_options = {'path': 's3://pi1-kjj/trusted/TSLA'},\n",
    "       format = 'parquet')\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "       frame = MSFT_dyf,\n",
    "       connection_type = 's3',\n",
    "       connection_options = {'path': 's3://pi1-kjj/trusted/MSFT'},\n",
    "       format = 'parquet')\n",
    "\n",
    "print('End storing to S3')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c748846c2dfb25129ebcfcdf87ddb016d7c05c80260d02b4445a617cbd972f89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
